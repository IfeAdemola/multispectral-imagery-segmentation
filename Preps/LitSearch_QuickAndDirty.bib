@article{baiMultispectralUNetSemantic,
  title = {Multispectral {{U-Net}}: {{A Semantic Segmentation Model Using Multispectral Bands Fusion Mechanism}} for {{Landslide Detection}}},
  author = {Bai, Lin and Li, Weile and Xu, Qiang and Peng, Weihang and Chen, Kai and Duan, Zhenzhen and Lu, Huiyan},
  abstract = {Each image patch of the Landslide4Sense dataset has 14 bands, which is different with the RGB satellite imagery. A new model called Multispectral U-Net was proposed. We conduct experiments and apply two modifications to traditional U-Net to improve the performance. Firstly, the skip-connection and spectral normalization regularization are used to enhance the model ability of feature extraction. Secondly, an extra Inverted Residuals and Linear Bottlenecks branch is introduced for 10 meters resolution bands. We split the official dataset into two parts, with 3539 images for training and 260 images for testing. Multispectral U-Net accomplishes the best performance, with an F1-score of 77.83\%, followed by the baseline U-Net and Deeplabv3+. The model has better performance on multi-spectral data and small objection for landslide detection.},
  langid = {english},
  file = {/home/magda/Zotero/storage/YZK69BUX/Bai et al. - Multispectral U-Net A Semantic Segmentation Model.pdf}
}

@incollection{caronDeepClusteringUnsupervised2018,
  title = {Deep {{Clustering}} for {{Unsupervised Learning}} of {{Visual Features}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2018},
  author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11218},
  pages = {139--156},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01264-9_9},
  urldate = {2024-01-22},
  abstract = {Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large-scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, kmeans, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.},
  isbn = {978-3-030-01263-2 978-3-030-01264-9},
  langid = {english},
  file = {/home/magda/Zotero/storage/C5BBJJV4/Caron et al. - 2018 - Deep Clustering for Unsupervised Learning of Visua.pdf}
}

@misc{eliasofUnsupervisedImageSemantic2022,
  title = {Unsupervised {{Image Semantic Segmentation}} through {{Superpixels}} and {{Graph Neural Networks}}},
  author = {Eliasof, Moshe and Zikri, Nir Ben and Treister, Eran},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11810},
  eprint = {2210.11810},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.11810},
  urldate = {2024-01-20},
  abstract = {Unsupervised image segmentation is an important task in many real-world scenarios where labelled data is of scarce availability. In this paper we propose a novel approach that harnesses recent advances in unsupervised learning using a combination of Mutual Information Maximization (MIM), Neural Superpixel Segmentation and Graph Neural Networks (GNNs) in an end-to-end manner, an approach that has not been explored yet. We take advantage of the compact representation of superpixels and combine it with GNNs in order to learn strong and semantically meaningful representations of images. Specifically, we show that our GNN based approach allows to model interactions between distant pixels in the image and serves as a strong prior to existing CNNs for an improved accuracy. Our experiments reveal both the qualitative and quantitative advantages of our approach compared to current state-of-the-art methods over four popular datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-01-20]},
  file = {/home/magda/Zotero/storage/2RYDNMFU/Eliasof et al_2022_Unsupervised Image Semantic Segmentation through Superpixels and Graph Neural.pdf;/home/magda/Zotero/storage/KWVSY3K8/2210.html}
}

@article{ertenSemanticSegmentationHighResolution2023,
  title = {Semantic {{Segmentation}} with {{High-Resolution Sentinel-1 SAR Data}}},
  author = {Erten, Hakan and Bostanci, Erkan and Acici, Koray and Guzel, Mehmet Serdar and Asuroglu, Tunc and Aydin, Ayhan},
  year = {2023},
  month = jan,
  journal = {Applied Sciences},
  volume = {13},
  number = {10},
  pages = {6025},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app13106025},
  urldate = {2024-01-20},
  abstract = {The world's high-resolution images are supplied by a radar system named Synthetic Aperture Radar (SAR). Semantic SAR image segmentation proposes a computer-based solution to make segmentation tasks easier. When conducting scientific research, accessing freely available datasets and images with low noise levels is rare. However, SAR images can be accessed for free. We propose a novel process for labeling Sentinel-1 SAR radar images, which the European Space Agency (ESA) provides free of charge. This process involves denoising the images and using an automatically created dataset with pioneering deep neural networks to augment the results of the semantic segmentation task. In order to exhibit the power of our denoising process, we match the results of our newly created dataset with speckled noise and noise-free versions. Thus, we attained a mean intersection over union (mIoU) of 70.60\% and overall pixel accuracy (PA) of 92.23 with the HRNet model. These deep learning segmentation methods were also assessed with the McNemar test. Our experiments on the newly created Sentinel-1 dataset establish that combining our pipeline with deep neural networks results in recognizable improvements in challenging semantic segmentation accuracy and mIoU values.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/XL88JDGQ/Erten et al_2023_Semantic Segmentation with High-Resolution Sentinel-1 SAR Data.pdf}
}

@inproceedings{girardClusterNetUnsupervisedGeneric2019,
  title = {{{ClusterNet}}: {{Unsupervised Generic Feature Learning}} for {{Fast Interactive Satellite Image Segmentation}}},
  shorttitle = {{{ClusterNet}}},
  booktitle = {Image and {{Signal Processing}} for {{Remote Sensing}} ({{SPIE}})},
  author = {Girard, Nicolas and Zhygallo, Andrii and Tarabalka, Yuliya},
  year = {2019},
  month = sep,
  urldate = {2024-01-20},
  abstract = {Semantic segmentation on satellite images is used to automatically detect and classify objects of interest over very large areas. Training a neural network for this task generally requires a lot of human-made ground truth classification masks for each object class of interest. We aim to reduce the time spent by humans in the whole process of image segmentation by learning generic features in an unsupervised manner. Those features are then used to leverage sparse human annotations to compute a dense segmentation of the image. This is achieved by essentially labeling groups of semantically similar pixels at once, instead of labeling each pixel almost individually using strokes. While we apply this method to satellite images, our approach is generic and can be applied to any image and to any class of objects on that image.},
  langid = {english},
  file = {/home/magda/Zotero/storage/LCZPJAVQ/Girard et al_2019_ClusterNet.pdf}
}

@misc{hamiltonUnsupervisedSemanticSegmentation2022,
  title = {Unsupervised {{Semantic Segmentation}} by {{Distilling Feature Correspondences}}},
  author = {Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T.},
  year = {2022},
  month = mar,
  number = {arXiv:2203.08414},
  eprint = {2203.08414},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2024-01-22},
  abstract = {Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO (Self-supervised Transformer with Energy-based Graph Optimization), a novel framework that distills unsupervised features into highquality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their relationships across the corpora. STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff (+14 mIoU) and Cityscapes (+9 mIoU) semantic segmentation challenges.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {133 citations (Semantic Scholar/arXiv) [2024-01-26]},
  file = {/home/magda/Zotero/storage/WMBRHKPE/Hamilton et al. - 2022 - Unsupervised Semantic Segmentation by Distilling F.pdf}
}

@misc{ibrahimliIibrahimliUnet_sem_seg2023,
  title = {Iibrahimli/Unet\_sem\_seg},
  author = {Ibrahimli, Imran},
  year = {2023},
  month = nov,
  urldate = {2024-01-20},
  abstract = {Semantic segmentation of satellite imagery using U-nets (U-nets: https://arxiv.org/abs/1505.04597)}
}

@article{kakogeorgiouEvaluatingExplainableArtificial2021,
  title = {Evaluating Explainable Artificial Intelligence Methods for Multi-Label Deep Learning Classification Tasks in Remote Sensing},
  author = {Kakogeorgiou, Ioannis and Karantzalos, Konstantinos},
  year = {2021},
  month = dec,
  journal = {International Journal of Applied Earth Observation and Geoinformation},
  volume = {103},
  eprint = {2104.01375},
  primaryclass = {cs},
  pages = {102520},
  issn = {15698432},
  doi = {10.1016/j.jag.2021.102520},
  urldate = {2024-01-20},
  abstract = {Although deep neural networks hold the state-of-the-art in several remote sensing tasks, their black-box operation hinders the understanding of their decisions, concealing any bias and other shortcomings in datasets and model performance. To this end, we have applied explainable artificial intelligence (XAI) methods in remote sensing multi-label classification tasks towards producing human-interpretable explanations and improve transparency. In particular, we utilized and trained deep learning models with state-of-the-art performance in the benchmark BigEarthNet and SEN12MS datasets. Ten XAI methods were employed towards understanding and interpreting models' predictions, along with quantitative metrics to assess and compare their performance. Numerous experiments were performed to assess the overall performance of XAI methods for straightforward prediction cases, competing multiple labels, as well as misclassification cases. According to our findings, Occlusion, Grad-CAM and Lime were the most interpretable and reliable XAI methods. However, none delivers high-resolution outputs, while apart from Grad-CAM, both Lime and Occlusion are computationally expensive. We also highlight different aspects of XAI performance and elaborate with insights on black-box decisions in order to improve transparency, understand their behavior and reveal, as well, datasets' particularities.},
  archiveprefix = {arxiv},
  annotation = {65 citations (Semantic Scholar/arXiv) [2024-01-20] 65 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/83Y3F9XK/Kakogeorgiou_Karantzalos_2021_Evaluating explainable artificial intelligence methods for multi-label deep.pdf;/home/magda/Zotero/storage/UH6QI3PW/2104.html}
}

@misc{kimCausalUnsupervisedSemantic2023,
  title = {Causal {{Unsupervised Semantic Segmentation}}},
  author = {Kim, Junho and Lee, Byung-Kwan and Ro, Yong Man},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07379},
  eprint = {2310.07379},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.07379},
  urldate = {2024-01-20},
  abstract = {Unsupervised semantic segmentation aims to achieve high-quality semantic grouping without human-labeled annotations. With the advent of self-supervised pre-training, various frameworks utilize the pre-trained features to train prediction heads for unsupervised dense prediction. However, a significant challenge in this unsupervised setup is determining the appropriate level of clustering required for segmenting concepts. To address it, we propose a novel framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages insights from causal inference. Specifically, we bridge intervention-oriented approach (i.e., frontdoor adjustment) to define suitable two-step tasks for unsupervised prediction. The first step involves constructing a concept clusterbook as a mediator, which represents possible concept prototypes at different levels of granularity in a discretized form. Then, the mediator establishes an explicit link to the subsequent concept-wise self-supervised learning for pixel-level grouping. Through extensive experiments and analyses on various datasets, we corroborate the effectiveness of CAUSE and achieve state-of-the-art performance in unsupervised semantic segmentation.},
  archiveprefix = {arxiv},
  annotation = {2 citations (Semantic Scholar/arXiv) [2024-01-20] 2 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/S6M269GZ/Kim et al_2023_Causal Unsupervised Semantic Segmentation.pdf;/home/magda/Zotero/storage/SCXVRBHX/2310.html}
}

@misc{liACSegAdaptiveConceptualization2023,
  title = {{{ACSeg}}: {{Adaptive Conceptualization}} for {{Unsupervised Semantic Segmentation}}},
  shorttitle = {{{ACSeg}}},
  author = {Li, Kehan and Wang, Zhennan and Cheng, Zesen and Yu, Runyi and Zhao, Yian and Song, Guoli and Liu, Chang and Yuan, Li and Chen, Jie},
  year = {2023},
  month = mar,
  number = {arXiv:2210.05944},
  eprint = {2210.05944},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.05944},
  urldate = {2024-01-20},
  abstract = {Recently, self-supervised large-scale visual pre-training models have shown great promise in representing pixel-level semantic relationships, significantly promoting the development of unsupervised dense prediction tasks, e.g., unsupervised semantic segmentation (USS). The extracted relationship among pixel-level representations typically contains rich class-aware information that semantically identical pixel embeddings in the representation space gather together to form sophisticated concepts. However, leveraging the learned models to ascertain semantically consistent pixel groups or regions in the image is non-trivial since over/ under-clustering overwhelms the conceptualization procedure under various semantic distributions of different images. In this work, we investigate the pixel-level semantic aggregation in self-supervised ViT pre-trained models as image Segmentation and propose the Adaptive Conceptualization approach for USS, termed ACSeg. Concretely, we explicitly encode concepts into learnable prototypes and design the Adaptive Concept Generator (ACG), which adaptively maps these prototypes to informative concepts for each image. Meanwhile, considering the scene complexity of different images, we propose the modularity loss to optimize ACG independent of the concept number based on estimating the intensity of pixel pairs belonging to the same concept. Finally, we turn the USS task into classifying the discovered concepts in an unsupervised manner. Extensive experiments with state-of-the-art results demonstrate the effectiveness of the proposed ACSeg.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {9 citations (Semantic Scholar/arXiv) [2024-01-20] 3 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/U7IGRA3S/Li et al_2023_ACSeg.pdf;/home/magda/Zotero/storage/SAREFA66/2210.html}
}

@article{liOneModelEnough2023,
  title = {One {{Model Is Enough}}: {{Toward Multiclass Weakly Supervised Remote Sensing Image Semantic Segmentation}}},
  shorttitle = {One {{Model Is Enough}}},
  author = {Li, Zhenshi and Zhang, Xueliang and Xiao, Pengfeng},
  year = {2023},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {61},
  pages = {1--13},
  issn = {0196-2892, 1558-0644},
  doi = {10.1109/TGRS.2023.3290242},
  urldate = {2024-01-20},
  abstract = {Semantic segmentation of remote sensing images (RSIs) is effective for large-scale land cover mapping, which heavily relies on a large amount of training data with laborious pixel-level labeling. Due to the easy availability of image-level labels, weakly supervised semantic segmentation (WSSS) based on them has attracted intensive attention. However, existing image-level WSSS methods for RSIs mainly focus on binary segmentation, which are difficult to apply to multiclass scenarios. This study proposes a comprehensive framework for image-level multiclass WSSS of RSIs, consisting of appropriate image-level label generation, high-quality pixel-level pseudo mask generation, and segmentation network iterative training. Specifically, a training sample filtering method, as well as a dataset co-occurrence evaluation metric, is proposed to demonstrate proper image-level training samples. Leveraging multiclass class activation maps (CAMs), an uncertainty-driven pixel-level weighted mask is proposed to relieve the overfitting of labeling noise in pseudo masks when training the segmentation network. Extensive experiments demonstrate that the proposed framework can achieve high-quality multiclass WSSS performance with image-level labels, which can attain 94.23\% and 90.77\% of the mean intersection over union (mIoU) from pixel-level labels for the ISPRS Potsdam and Vaihingen datasets, respectively. Beyond that, for the DeepGlobe dataset with more complex landscapes, the WSSS framework can achieve an accuracy close to 99\% of the fully supervised case. In addition, we further demonstrate that compared to adopting multiple binary WSSS models, directly training a multiclass WSSS model can achieve better results, which can provide new thoughts to achieve WSSS of RSIs for multiclass application scenarios. Our code is publically available at https://github.com/NJU-LHRS/OME.},
  langid = {english},
  annotation = {3 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/EUGXUR3G/Li et al. - 2023 - One Model Is Enough Toward Multiclass Weakly Supe.pdf}
}

@misc{LUSSegAwesomeUnsupervisedSemanticSegmentation2024,
  title = {{{LUSSeg}}/{{Awesome-Unsupervised-Semantic-Segmentation}}},
  year = {2024},
  month = jan,
  urldate = {2024-01-20},
  abstract = {A summary of recent unsupervised semantic segmentation methods},
  howpublished = {Large-scale Unsupervised Semantic Segmentation}
}

@inproceedings{melas-kyriaziDeepSpectralMethods2022,
  title = {Deep {{Spectral Methods}}: {{A Surprisingly Strong Baseline}} for {{Unsupervised Semantic Segmentation}} and {{Localization}}},
  shorttitle = {Deep {{Spectral Methods}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Melas-Kyriazi}, Luke and Rupprecht, Christian and Laina, Iro and Vedaldi, Andrea},
  year = {2022},
  pages = {8364--8375},
  urldate = {2024-01-20},
  langid = {english},
  keywords = {todo},
  file = {/home/magda/Zotero/storage/3QJ75N9C/Melas-Kyriazi et al_2022_Deep Spectral Methods.pdf}
}

@article{muhtarIndexYourPosition2022,
  title = {Index {{Your Position}}: {{A Novel Self-Supervised Learning Method}} for {{Remote Sensing Images Semantic Segmentation}}},
  shorttitle = {Index {{Your Position}}},
  author = {Muhtar, Dilxat and Zhang, Xueliang and Xiao, Pengfeng},
  year = {2022},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {60},
  pages = {1--11},
  issn = {0196-2892, 1558-0644},
  doi = {10.1109/TGRS.2022.3177770},
  urldate = {2024-01-20},
  abstract = {Learning effective visual representations without human supervision is a critical problem for the task of semantic segmentation of remote sensing images (RSIs), where pixel-level annotations are difficult to obtain. Self-supervised learning (SSL), which learns useful representations by creating artificial supervised learning problems, has recently emerged as an effective method to learn from unlabelled data. Current SSL methods are generally trained on ImageNet through image-level prediction tasks. We argue that this is suboptimal for application in semantic segmentation of RSIs since it does not take into account spatial position information between objects, which is critical for the segmentation of RSIs characterized by multiobject. In this study, we propose a novel self-supervised dense representation learning method, IndexNet, for the semantic segmentation of RSIs. On the one hand, considering the multiobject characteristics of RSIs, IndexNet learns pixel-level representations by tracking object positions, while maintaining sensitivity to object position changes to ensure that no mismatches are caused. On the other hand, by combining image-level contrast and pixel-level contrast, IndexNet can learn spatiotemporal invariant features. Experimental results show that our method works better than ImageNet pretraining and outperforms state-of-the-art (SOTA) SSL methods. Code and pretrained models will be available at https://github.com/pUmpKin-Co/offical-IndexNet.},
  langid = {english},
  annotation = {20 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/TKYMZP4K/Muhtar et al. - 2022 - Index Your Position A Novel Self-Supervised Learn.pdf}
}

@misc{pelaez-vegasSurveySemiSupervisedSemantic2023,
  title = {A {{Survey}} on {{Semi-Supervised Semantic Segmentation}}},
  author = {{Pel{\'a}ez-Vegas}, Adrian and Mesejo, Pablo and Luengo, Juli{\'a}n},
  year = {2023},
  month = feb,
  number = {arXiv:2302.09899},
  eprint = {2302.09899},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.09899},
  urldate = {2024-01-20},
  abstract = {Semantic segmentation is one of the most challenging tasks in computer vision. However, in many applications, a frequent obstacle is the lack of labeled images, due to the high cost of pixel-level labeling. In this scenario, it makes sense to approach the problem from a semi-supervised point of view, where both labeled and unlabeled images are exploited. In recent years this line of research has gained much interest and many approaches have been published in this direction. Therefore, the main objective of this study is to provide an overview of the current state of the art in semi-supervised semantic segmentation, offering an updated taxonomy of all existing methods to date. This is complemented by an experimentation with a variety of models representing all the categories of the taxonomy on the most widely used becnhmark datasets in the literature, and a final discussion on the results obtained, the challenges and the most promising lines of future research.},
  archiveprefix = {arxiv},
  annotation = {6 citations (Semantic Scholar/arXiv) [2024-01-20] 6 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/KQIRTGB3/Pel√°ez-Vegas et al_2023_A Survey on Semi-Supervised Semantic Segmentation.pdf;/home/magda/Zotero/storage/6YP2MG9W/2302.html}
}

@article{regmiUnsupervisedImageSegmentation2023,
  title = {Unsupervised Image Segmentation in Satellite Imagery Using Deep Learning},
  author = {Regmi, Suraj},
  year = {2023},
  month = jan,
  journal = {Theses},
  file = {/home/magda/Zotero/storage/WCAAAHEQ/Regmi - Unsupervised image segmentation in satellite image.pdf;/home/magda/Zotero/storage/XFJCPI6X/458.html}
}

@misc{schmittWeaklySupervisedSemantic2020a,
  title = {Weakly {{Supervised Semantic Segmentation}} of {{Satellite Images}} for {{Land Cover Mapping}} -- {{Challenges}} and {{Opportunities}}},
  author = {Schmitt, Michael and Prexl, Jonathan and Ebel, Patrick and Liebel, Lukas and Zhu, Xiao Xiang},
  year = {2020},
  month = apr,
  number = {arXiv:2002.08254},
  eprint = {2002.08254},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.08254},
  urldate = {2024-01-20},
  abstract = {Fully automatic large-scale land cover mapping belongs to the core challenges addressed by the remote sensing community. Usually, the basis of this task is formed by (supervised) machine learning models. However, in spite of recent growth in the availability of satellite observations, accurate training data remains comparably scarce. On the other hand, numerous global land cover products exist and can be accessed often free-of-charge. Unfortunately, these maps are typically of a much lower resolution than modern day satellite imagery. Besides, they always come with a significant amount of noise, as they cannot be considered ground truth, but are products of previous (semi-)automatic prediction tasks. Therefore, this paper seeks to make a case for the application of weakly supervised learning strategies to get the most out of available data sources and achieve progress in high-resolution large-scale land cover mapping. Challenges and opportunities are discussed based on the SEN12MS dataset, for which also some baseline results are shown. These baselines indicate that there is still a lot of potential for dedicated approaches designed to deal with remote sensing-specific forms of weak supervision.},
  archiveprefix = {arxiv},
  annotation = {46 citations (Semantic Scholar/arXiv) [2024-01-20]},
  file = {/home/magda/Zotero/storage/7R5AZ2NR/Schmitt et al_2020_Weakly Supervised Semantic Segmentation of Satellite Images for Land Cover.pdf;/home/magda/Zotero/storage/KUMGL74D/2002.html}
}

@article{sunBGFNetSemanticSegmentation2024,
  title = {{{BGFNet}}: {{Semantic Segmentation Network Based}} on {{Boundary Guidance}}},
  shorttitle = {{{BGFNet}}},
  author = {Sun, Xiao and Qian, Yurong and Cao, Ruyi and Tuerxun, Palidan and Hu, Zhehao},
  year = {2024},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  volume = {21},
  pages = {1--5},
  issn = {1558-0571},
  doi = {10.1109/LGRS.2023.3333017},
  urldate = {2024-01-20},
  abstract = {Over the past few years, there have been significant advancements in deep learning technology, leading to remarkable progress in the field of image analysis. However, when it comes to handling complex remote sensing images, current semantic segmentation methods still face challenges and do not perform as well as desired. How to obtain both spatial detail information and semantic information at the same time is an urgent problem to be solved. This letter proposes a context fusion network based on boundary guidance (BGFNet), which incorporates the patch attention module (PAM), the feature maps are enriched with contextual information, improving their ability to capture spatial dependencies. In order to alleviate boundary ambiguity, a boundary guidance module (BGM) is used to weight features with rich semantic boundary information. Furthermore, the compatible fusion module (CFM) is employed to merge high-order and low-order features, creating novel features. Channel attention is then applied to the obtained features allows us to select the desired features by filtering out irrelevant information. We validate our model on the Vaihingen and Potsdam datasets reached 81.65\% and 86.94\% mean intersection over union (mIoU), respectively, indicating the superiority of the proposed model.},
  annotation = {0 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/FVPS3A6Z/Sun et al_2024_BGFNet.pdf;/home/magda/Zotero/storage/BW46BMCM/10318828.html}
}

@article{taoMSNetMultispectralSemantic2022,
  title = {{{MSNet}}: Multispectral Semantic Segmentation Network for Remote Sensing Images},
  shorttitle = {{{MSNet}}},
  author = {Tao, Chongxin and Meng, Yizhuo and Li, Junjie and Yang, Beibei and Hu, Fengmin and Li, Yuanxi and Cui, Changlu and Zhang, Wen},
  year = {2022},
  month = dec,
  journal = {GIScience \& Remote Sensing},
  volume = {59},
  number = {1},
  pages = {1177--1198},
  publisher = {{Taylor \& Francis}},
  issn = {1548-1603},
  doi = {10.1080/15481603.2022.2101728},
  urldate = {2024-01-20},
  abstract = {In the research of automatic interpretation of remote sensing images, semantic segmentation based on deep convolutional neural networks has been rapidly developed and applied, and the feature segmentation accuracy and network model generalization ability have been gradually improved. However, most of the network designs are mainly oriented to the three visible RGB bands of remote sensing images, aiming to be able to directly borrow the mature natural image semantic segmentation networks and pre-trained models, but simultaneously causing the waste and loss of spectral information in the invisible light bands such as near-infrared (NIR) of remote sensing images. Combining the advantages of multispectral data in distinguishing typical features such as water and vegetation, we propose a novel deep neural network structure called the multispectral semantic segmentation network (MSNet) for semantic segmentation of multi-classified feature scenes. The multispectral remote sensing image bands are split into two groups, visible and invisible, and ResNet-50 is used for feature extraction in both coding stages, and cascaded upsampling is used to recover feature map resolution in the decoding stage, and the multi-scale image features and spectral features from the upsampling process are fused layer by layer using the feature pyramid structure to finally obtain semantic segmentation results. The training and validation results on two publicly available datasets show that MSNet has competitive performance. The code is available: https://github.com/taochx/MSNet.},
  annotation = {11 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/SHSWP9EN/Tao et al_2022_MSNet.pdf}
}

@article{tzepkenlisEfficientDeepSemantic2023,
  title = {Efficient {{Deep Semantic Segmentation}} for {{Land Cover Classification Using Sentinel Imagery}}},
  author = {Tzepkenlis, Anastasios and Marthoglou, Konstantinos and Grammalidis, Nikos},
  year = {2023},
  month = jan,
  journal = {Remote Sensing},
  volume = {15},
  number = {8},
  pages = {2027},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2072-4292},
  doi = {10.3390/rs15082027},
  urldate = {2024-01-20},
  abstract = {Nowadays, different machine learning approaches, either conventional or more advanced, use input from different remote sensing imagery for land cover classification and associated decision making. However, most approaches rely heavily on time-consuming tasks to gather accurate annotation data. Furthermore, downloading and pre-processing remote sensing imagery used to be a difficult and time-consuming task that discouraged policy makers to create and use new land cover maps. We argue that by combining recent improvements in deep learning with the use of powerful cloud computing platforms for EO data processing, specifically the Google Earth Engine, we can greatly facilitate the task of land cover classification. For this reason, we modify an efficient semantic segmentation approach (U-TAE) for a satellite image time series to use, as input, a single multiband image composite corresponding to a specific time range. Our motivation is threefold: (a) to improve land cover classification performance and at the same time reduce complexity by using, as input, satellite image composites with reduced noise created using temporal median instead of the original noisy (due to clouds, calibration errors, etc.) images, (b) to assess performance when using as input different combinations of satellite data, including Sentinel-2, Sentinel-1, spectral indices, and ALOS elevation data, and (c) to exploit channel attention instead of the temporal attention used in the original approach. We show that our proposed modification on U-TAE (mIoU: 57.25\%) outperforms three other popular approaches, namely random forest (mIoU: 39.69\%), U-Net (mIoU: 55.73\%), and SegFormer (mIoU: 53.5\%), while also using fewer training parameters. In addition, the evaluation reveals that proper selection of the input band combination is necessary for improved performance.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  annotation = {8 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/PX4FHBSL/Tzepkenlis et al_2023_Efficient Deep Semantic Segmentation for Land Cover Classification Using.pdf}
}

@inproceedings{uhlemeyerUnsupervisedOpenWorld2022,
  title = {Towards Unsupervised Open World Semantic Segmentation},
  booktitle = {Proceedings of the {{Thirty-Eighth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Uhlemeyer, Svenja and Rottmann, Matthias and Gottschalk, Hanno},
  year = {2022},
  month = aug,
  pages = {1981--1991},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2024-01-20},
  abstract = {For the semantic segmentation of images, state-of-the-art deep neural networks (DNNs) achieve high segmentation accuracy if that task is restricted to a closed set of classes. However, as of now DNNs have limited ability to operate in an open world, where they are tasked to identify pixels belonging to unknown objects and eventually to learn novel classes, incrementally. Humans have the capability to say: ``I don't know what that is, but I've already seen something like that''. Therefore, it is desirable to perform such an incremental learning task in an unsupervised fashion. We introduce a method where unknown objects are clustered based on visual similarity. Those clusters are utilized to define new classes and serve as training data for unsupervised incremental learning. More precisely, the connected components of a predicted semantic segmentation are assessed by a segmentation quality estimate. Connected components with a low estimated prediction quality are candidates for a subsequent clustering. Additionally, the component-wise quality assessment allows for obtaining predicted segmentation masks for the image regions potentially containing unknown objects. The respective pixels of such masks are pseudo-labeled and afterwards used for re-training the DNN, i.e., without the use of ground truth generated by humans. In our experiments we demonstrate that, without access to ground truth and even with few data, a DNN's class space can be extended by a novel class, achieving considerable segmentation accuracy.},
  langid = {english},
  file = {/home/magda/Zotero/storage/7LAZRCXI/Uhlemeyer et al. - 2022 - Towards unsupervised open world semantic segmentat.pdf;/home/magda/Zotero/storage/FB4KPXTJ/Uhlemeyer et al_2022_Towards unsupervised open world semantic segmentation.pdf}
}

@misc{UnsupervisedSegmentationSatellite,
  title = {Unsupervised Segmentation of~Satellite Images},
  urldate = {2024-01-20},
  howpublished = {https://www.nextml.com/projects/unsupervised-segmentation-of-satellite-images}
}

@article{yaoMultiCategorySegmentationSentinel22022,
  title = {Multi-{{Category Segmentation}} of {{Sentinel-2 Images Based}} on the {{Swin UNet Method}}},
  author = {Yao, Junyuan and Jin, Shuanggen},
  year = {2022},
  month = jan,
  journal = {Remote Sensing},
  volume = {14},
  number = {14},
  pages = {3382},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2072-4292},
  doi = {10.3390/rs14143382},
  urldate = {2024-01-20},
  abstract = {Medium-resolution remote sensing satellites have provided a large amount of long time series and full coverage data for Earth surface monitoring. However, the different objects may have similar spectral values and the same objects may have different spectral values, which makes it difficult to improve the classification accuracy. Semantic segmentation of remote sensing images is greatly facilitated via deep learning methods. For medium-resolution remote sensing images, the convolutional neural network-based model does not achieve good results due to its limited field of perception. The fast-emerging vision transformer method with self-attentively capturing global features well provides a new solution for medium-resolution remote sensing image segmentation. In this paper, a new multi-class segmentation method is proposed for medium-resolution remote sensing images based on the improved Swin UNet model as a pure transformer model and a new pre-processing, and the image enhancement method and spectral selection module are designed to achieve better accuracy. Finally, 10-categories segmentation is conducted with 10-m resolution Sentinel-2 MSI (Multi-Spectral Imager) images, which is compared with other traditional convolutional neural network-based models (DeepLabV3+ and U-Net with different backbone networks, including VGG, ResNet50, MobileNet, and Xception) with the same sample data, and results show higher Mean Intersection Over Union (MIOU) (72.06\%) and better accuracy (89.77\%) performance. The vision transformer method has great potential for medium-resolution remote sensing image segmentation tasks.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  annotation = {13 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/FXKYXYYF/Yao_Jin_2022_Multi-Category Segmentation of Sentinel-2 Images Based on the Swin UNet Method.pdf}
}

@article{yuDeepLearningMethods2023,
  title = {Deep {{Learning Methods}} for {{Semantic Segmentation}} in {{Remote Sensing}} with {{Small Data}}: {{A Survey}}},
  shorttitle = {Deep {{Learning Methods}} for {{Semantic Segmentation}} in {{Remote Sensing}} with {{Small Data}}},
  author = {Yu, Anzhu and Quan, Yujun and Yu, Ru and Guo, Wenyue and Wang, Xin and Hong, Danyang and Zhang, Haodi and Chen, Junming and Hu, Qingfeng and He, Peipei},
  year = {2023},
  month = jan,
  journal = {Remote Sensing},
  volume = {15},
  number = {20},
  pages = {4987},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2072-4292},
  doi = {10.3390/rs15204987},
  urldate = {2024-01-20},
  abstract = {The annotations used during the training process are crucial for the inference results of remote sensing images (RSIs) based on a deep learning framework. Unlabeled RSIs can be obtained relatively easily. However, pixel-level annotation is a process that necessitates a high level of expertise and experience. Consequently, the use of small sample training methods has attracted widespread attention as they help alleviate reliance on large amounts of high-quality labeled data and current deep learning methods. Moreover, research on small sample learning is still in its infancy owing to the unique challenges faced when completing semantic segmentation tasks with RSI. To better understand and stimulate future research that utilizes semantic segmentation tasks with small data, we summarized the supervised learning methods and challenges they face. We also reviewed the supervised approaches with data that are currently popular to help elucidate how to efficiently utilize a limited number of samples to address issues with semantic segmentation in RSI. The main methods discussed are self-supervised learning, semi-supervised learning, weakly supervised learning and few-shot methods. The solution of cross-domain challenges has also been discussed. Furthermore, multi-modal methods, prior knowledge constrained methods, and future research required to help optimize deep learning models for various downstream tasks in relation to RSI have been identified.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/9MW2XHD7/Yu et al_2023_Deep Learning Methods for Semantic Segmentation in Remote Sensing with Small.pdf}
}

@misc{zadaianchukUnsupervisedSemanticSegmentation2023,
  title = {Unsupervised {{Semantic Segmentation}} with {{Self-supervised Object-centric Representations}}},
  author = {Zadaianchuk, Andrii and Kleindessner, Matthaeus and Zhu, Yi and Locatello, Francesco and Brox, Thomas},
  year = {2023},
  month = apr,
  number = {arXiv:2207.05027},
  eprint = {2207.05027},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.05027},
  urldate = {2024-01-20},
  abstract = {In this paper, we show that recent advances in self-supervised feature learning enable unsupervised object discovery and semantic segmentation with a performance that matches the state of the field on supervised semantic segmentation 10 years ago. We propose a methodology based on unsupervised saliency masks and self-supervised feature clustering to kickstart object discovery followed by training a semantic segmentation network on pseudo-labels to bootstrap the system on images with multiple objects. We present results on PASCAL VOC that go far beyond the current state of the art (50.0 mIoU), and we report for the first time results on MS COCO for the whole set of 81 classes: our method discovers 34 categories with more than \$20{\textbackslash}\%\$ IoU, while obtaining an average IoU of 19.6 for all 81 categories.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {28 citations (Semantic Scholar/arXiv) [2024-01-20] 28 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/84BI382Y/Zadaianchuk et al_2023_Unsupervised Semantic Segmentation with Self-supervised Object-centric.pdf;/home/magda/Zotero/storage/52ANKH2U/2207.html}
}

@article{zhangBoostingSemanticSegmentation2023,
  title = {Boosting {{Semantic Segmentation}} of {{Remote Sensing Images}} by {{Introducing Edge Extraction Network}} and {{Spectral Indices}}},
  author = {Zhang, Yue and Yang, Ruiqi and Dai, Qinling and Zhao, Yili and Xu, Weiheng and Wang, Jun and Wang, Leiguang},
  year = {2023},
  month = jan,
  journal = {Remote Sensing},
  volume = {15},
  number = {21},
  pages = {5148},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2072-4292},
  doi = {10.3390/rs15215148},
  urldate = {2024-01-20},
  abstract = {Deep convolutional neural networks have greatly enhanced the semantic segmentation of remote sensing images. However, most networks are primarily designed to process imagery with red, green, and blue bands. Although it is feasible to directly utilize established networks and pre-trained models for remotely sensed images, they suffer from imprecise land object contour localization and unsatisfactory segmentation results. These networks still need to explore the domain knowledge embedded in images. Therefore, we boost the segmentation performance of remote sensing images by augmenting the network input with multiple nonlinear spectral indices, such as vegetation and water indices, and introducing a novel holistic attention edge detection network (HAE-RNet). Experiments were conducted on the GID and Vaihingen datasets. The results showed that the NIR-NDWI/DSM-GNDVI-R-G-B (6C-2) band combination produced the best segmentation results for both datasets. The edge extraction block benefits better contour localization. The proposed network achieved a state-of-the-art performance in both the quantitative evaluation and visual inspection.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/5FF8F38Q/Zhang et al_2023_Boosting Semantic Segmentation of Remote Sensing Images by Introducing Edge.pdf}
}

@inproceedings{zhangSOFUNetSAROptical2022,
  title = {{{SOF-UNet}}: {{SAR}} and {{Optical Fusion Unet}} for {{Land Cover Classification}}},
  shorttitle = {{{SOF-UNet}}},
  booktitle = {{{IGARSS}} 2022 - 2022 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Zhang, Di and Gade, Martin and Zhang, Jianwei},
  year = {2022},
  month = jul,
  pages = {907--910},
  issn = {2153-7003},
  doi = {10.1109/IGARSS46834.2022.9884504},
  urldate = {2024-01-20},
  abstract = {We propose a SAR and Optical Fusion Network based on the UNet framework (SOF-UNet) for multi-modal land cover classification. The two-stream SOF-UNet consists of three parts: two encoders to extract features, a sharing decoder to upsample the feature maps and specially designed skip connections to fuse multi-modal features. The qualitative and quantitative experimental results show that SOF-UNet has a promising capability to identify different land cover classes and can retain fine details in the prediction maps. Symmetric Cross Entropy (SCE) loss is also verified useful in this framework.},
  annotation = {0 citations (Semantic Scholar/DOI) [2024-01-20]},
  file = {/home/magda/Zotero/storage/5CGTRNHB/Zhang et al_2022_SOF-UNet.pdf;/home/magda/Zotero/storage/Q3QYNWMR/9884504.html}
}
