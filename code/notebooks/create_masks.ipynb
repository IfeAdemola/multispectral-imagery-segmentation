{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import shutil\n",
    "# from plot import * #load_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- For each region get the label of each timestamp\n",
    "- The mask in the region is (0,1) 0 being invalid areas, such as farmlands, 1 being valid areas such as forest area\n",
    "- Within the forest area, there are deforested regions\n",
    "- These regions are taken into consideration by creating a third mask, 2. The mask is generated using the mask channel (11th channel) in the region data, and the 1st and 2nd channels (ID, time stamp) in the areas without trees data\n",
    "- If a pixel is 1 in the region mask (meaning it is part of the valid region) and if it is greater than 0 in the ID channel of the without trees data (meaning it is a deforested area/clearing) and if it is a specific date based on the file name, the clearing was created at that time stamp. \n",
    "- I think I should do less than equal to to take into consideration the deforested areas that already occured in previous time steps and not just new ones in that time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load region data (x, y, 11) and without tree data (x, y, 4) files \n",
    "- Concatenate the 'id','timestamp' and 'veg cover' channels to all 'region_..._final_data' arrays\n",
    "- 'region_..._final_data' arrays now have a shape of (x, y, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not apppend areas without trees to dictionary so len should be 6 and not 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/k45848/multispectral-imagery-segmentation/data/region_2_east_2023_05_31_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/region_2_east_2024_02_25_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/areas_without_trees_east_raster_final.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/region_1_west_2023_09_26_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/region_1_west_2024_02_28_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/areas_without_trees_west_raster_final.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/region_2_east_2023_09_28_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/region_1_west_2023_05_31_final_data.pkl']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"/home/k45848/multispectral-imagery-segmentation/data\"\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".pkl\")]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_2_east_2023_05_31_final_data (746, 744, 15)\n",
      "region_2_east_2024_02_25_final_data (746, 744, 15)\n",
      "region_1_west_2023_09_26_final_data (931, 932, 15)\n",
      "region_1_west_2024_02_28_final_data (931, 932, 15)\n",
      "region_2_east_2023_09_28_final_data (746, 744, 15)\n",
      "region_1_west_2023_05_31_final_data (931, 932, 15)\n"
     ]
    }
   ],
   "source": [
    "def load_array(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "folder_path = \"/home/k45848/multispectral-imagery-segmentation/data\"\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".pkl\")]\n",
    "\n",
    "arrays = {}\n",
    "for path in file_paths:\n",
    "    array = load_array(path)\n",
    "\n",
    "    # Extract file name without extension as key\n",
    "    key = os.path.splitext(os.path.basename(path))[0]\n",
    "    \n",
    "    # Store array in dictionary\n",
    "    arrays[key] = array\n",
    "\n",
    "# Iterate through the dictionary keys\n",
    "for key in arrays.keys():\n",
    "    # Check if the key contains 'without_trees'\n",
    "    if 'without_trees' in key:\n",
    "        # Extract cardinal point from the key\n",
    "        cardinal_point = key.split('_')[-3]\n",
    "        \n",
    "        # Find matching keys without 'without_trees' and the same cardinal point\n",
    "        matching_keys = [k for k in arrays.keys() if cardinal_point in k and 'without_trees' not in k]\n",
    "        \n",
    "        # Iterate through matching keys and update arrays\n",
    "        for match_key in matching_keys:\n",
    "            # Add the first two channels from the 'without_trees' array to the matching array\n",
    "            arrays[match_key] = np.concatenate([arrays[match_key], arrays[key][...]], axis=-1)\n",
    "\n",
    "mask = 'without_trees'\n",
    "arrays = {key: value for key, value in arrays.items() if mask not in key}\n",
    "\n",
    "for key, value in arrays.items():\n",
    "    print(key, value.shape)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contradictry to the previous explanation, I concatenated all channels in the 'areas without trees' files to their corresponding regional image array\n",
    "- Shape = (x,y,15)\n",
    "- array[:,:10] = 10 spectral channels of image\n",
    "- array[10] = mask created by Denise 1 (valid: forest area), and 0 (invalid: urban areas)\n",
    "- array[11] = id (areas without trees)\n",
    "- array[12] = timestamp (areas without trees)\n",
    "- array[13] = type (areas without trees)\n",
    "- array[14] = veg_cover (areas without trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply condition to get the regions that are deforested for each time stamp\n",
    "- So now channel 11 has 3 unique values (0,1,2) rather than 2 (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2., 3.], dtype=float32), array([547678,   6745,    399,    202]))\n",
      "(array([0., 1., 2., 3.], dtype=float32), array([547678,   6745,    399,    202]))\n",
      "(array([0., 1., 2., 3., 4.], dtype=float32), array([860466,   4442,    908,    445,   1431]))\n",
      "(array([0., 1., 2., 3., 4.], dtype=float32), array([860466,   4442,    908,    445,   1431]))\n",
      "(array([0., 1., 2., 3.], dtype=float32), array([547678,   6745,    399,    202]))\n",
      "(array([0., 1., 2., 3., 4.], dtype=float32), array([860466,   4442,    908,    445,   1431]))\n"
     ]
    }
   ],
   "source": [
    "for key, value in arrays.items():\n",
    "    print(np.unique(value[...,14], return_counts=True))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(arrays.items())#[:-1]\n",
    "\n",
    "# Add deforested class 2 to channel 11\n",
    "for key, array in items:\n",
    "    time_stamp_match = re.search(r'(\\d{4})_(\\d{2})_(\\d{2})', key)\n",
    "\n",
    "    if time_stamp_match:\n",
    "        ts = float(time_stamp_match.group(1)[2:] + time_stamp_match.group(2) + time_stamp_match.group(3))\n",
    "\n",
    "    condition = (array[..., 10] == 1) & (array[..., 11] > 0) & (array[..., 12] <= ts)\n",
    "    \n",
    "    array[condition, 10] = 2\n",
    "\n",
    "\n",
    "    # print(np.unique(array[..., -1], return_counts=True))\n",
    "\n",
    "    # Make a new channel where deforested class pixels are split into their different vegetation cover\n",
    "    # Add a new channel, initialise to zero\n",
    "    veg_mask = np.zeros(array.shape[:-1] +(1,))\n",
    "    array = np.concatenate((array, veg_mask), axis=-1)\n",
    "\n",
    "    condition_1 = (array[..., 10] == 1)\n",
    "    array[condition_1, 15] = 5\n",
    "\n",
    "    condition_2 = (array[..., 10] == 2) & (array[..., 14] == 1)\n",
    "    array[condition_2, 15] = 1\n",
    "\n",
    "    condition_3 = (array[..., 10] == 2) & (array[..., 14] == 2)\n",
    "    array[condition_3, 15] = 2\n",
    "\n",
    "    condition_4 = (array[..., 10] == 2) & (array[..., 14] == 3)\n",
    "    array[condition_4, 15] = 3\n",
    "\n",
    "    condition_5 = (array[..., 10] == 2) & (array[..., 14] == 4)\n",
    "    array[condition_5, 15] = 4\n",
    "\n",
    "    \"\"\"At this point array has 16 channels: 10 spectral, 1 mask, 4 attributes and 1 veg mask\"\"\"\n",
    "    # Create a new channel to take into account the weather season, making it 17 channels\n",
    "    if '2023_05'  in key:\n",
    "        season_mask = np.zeros(array.shape[:-1]+ (1,))\n",
    "    elif '2023_09' in key:\n",
    "        season_mask = np.ones(array.shape[:-1]+ (1,))\n",
    "    elif '2024_02' in key:\n",
    "        season_mask = np.full(array.shape[:-1]+ (1,), 2)     \n",
    "\n",
    "    array = np.concatenate((array, season_mask), axis=-1)\n",
    "\n",
    "    # Delete the attribute channels (id, timestamp, ...)\n",
    "    # Should have just 13 channels left\n",
    "    array = np.delete(array, [11,12,13,14], axis=-1)\n",
    "\n",
    "    arrays[key] = array\n",
    "\n",
    "    filename = f\"/home/k45848/multispectral-imagery-segmentation/data/clean_data/{key}.pkl\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(array, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check shape, of array, unique values e.t.c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 932, 13)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_2_east_2023_05_31_final_data (746, 744, 13)\n",
      "region_2_east_2024_02_25_final_data (746, 744, 13)\n",
      "region_1_west_2023_09_26_final_data (931, 932, 13)\n",
      "region_1_west_2024_02_28_final_data (931, 932, 13)\n",
      "region_2_east_2023_09_28_final_data (746, 744, 13)\n",
      "region_1_west_2023_05_31_final_data (931, 932, 13)\n"
     ]
    }
   ],
   "source": [
    "for key, value in arrays.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2.]), array([211064, 342283,   1677]))\n",
      "(array([0., 1., 2.]), array([245556, 302161,   7307]))\n",
      "(array([0., 1., 2.]), array([423854, 436952,   6886]))\n",
      "(array([0., 1., 2.]), array([421453, 439060,   7179]))\n",
      "(array([0., 1., 2.]), array([215878, 332337,   6809]))\n",
      "(array([0., 1., 2.]), array([421453, 442701,   3538]))\n",
      "(array([0., 1., 2., 3., 5.]), array([211064,   1159,    316,    202, 342283]))\n",
      "(array([0., 1., 2., 3., 5.]), array([245556,   6706,    399,    202, 302161]))\n",
      "(array([0., 1., 2., 3., 4., 5.]), array([423854,   4149,    861,    445,   1431, 436952]))\n",
      "(array([0., 1., 2., 3., 4., 5.]), array([421453,   4442,    861,    445,   1431, 439060]))\n",
      "(array([0., 1., 2., 3., 5.]), array([215878,   6208,    399,    202, 332337]))\n",
      "(array([0., 1., 2., 3., 4., 5.]), array([421453,   1011,    651,    445,   1431, 442701]))\n",
      "region_2_east_2023_05_31_final_data\n",
      "(array([0.]), array([555024]))\n",
      "region_2_east_2024_02_25_final_data\n",
      "(array([2.]), array([555024]))\n",
      "region_1_west_2023_09_26_final_data\n",
      "(array([1.]), array([867692]))\n",
      "region_1_west_2024_02_28_final_data\n",
      "(array([2.]), array([867692]))\n",
      "region_2_east_2023_09_28_final_data\n",
      "(array([1.]), array([555024]))\n",
      "region_1_west_2023_05_31_final_data\n",
      "(array([0.]), array([867692]))\n"
     ]
    }
   ],
   "source": [
    "for key, value in arrays.items():\n",
    "    print(np.unique(value[...,10], return_counts=True))\n",
    "\n",
    "for key, value in arrays.items():\n",
    "    print(np.unique(value[...,11], return_counts=True))\n",
    " \n",
    "for key, value in arrays.items():\n",
    "    print(key)\n",
    "    print(np.unique(value[...,12], return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split arrays into patches and distribute patches randomly to train and eval directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_patches(file_path: str, patch_size: int, stride: int) -> None:\n",
    "    \"\"\"\n",
    "    Convert an array to patches of a given size and stride and save them to train or eval directory.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the input array file.\n",
    "    - patch_size (int): Size of each patch.\n",
    "    - stride (int): Stride between patches.\n",
    "    - train_dir (str): Directory to save train patches.\n",
    "    - eval_dir (str): Directory to save eval patches.\n",
    "    \"\"\"\n",
    "    array = load_array(file_path)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]  # Get file name without extension\n",
    "\n",
    "    height, width, _ =  array.shape\n",
    "    patch_height, patch_width = patch_size, patch_size\n",
    "    stride_height, stride_width = stride, stride\n",
    "\n",
    "    # Calculate the number of patches in each dimension\n",
    "    num_patches_height = (height - patch_height) // stride_height + 1\n",
    "    num_patches_width = (width - patch_width) // stride_width + 1\n",
    "\n",
    "    # Determine the directory based on a random choice for train or eval\n",
    "    base_dir =os.path.dirname(file_path)\n",
    "    train_dir = f\"{base_dir}/train/\"\n",
    "    eval_dir = f\"{base_dir}/eval/\"\n",
    "\n",
    "    # dir = train_dir if random.choice([True, False]) else eval_dir\n",
    "    # Determine the directory based on the 80:20 split\n",
    "    dir = train_dir if random.random() < 0.8 else eval_dir\n",
    "    # os.makedirs(dir)\n",
    "\n",
    "    for i in range(num_patches_height):\n",
    "        for j in range(num_patches_width):\n",
    "            start_i = i * stride_height\n",
    "            start_j = j * stride_width\n",
    "            patch = array[start_i:start_i + patch_height, start_j:start_j + patch_width, :]\n",
    "\n",
    "            patch_filename = f\"{dir}{file_name}_{i* num_patches_width + j + 1}.pkl\"\n",
    "             \n",
    "            with open(patch_filename, 'wb') as f:\n",
    "                pickle.dump(patch, f)\n",
    "\n",
    "def count_files_in_directory(directory: str) -> int:\n",
    "    return len([name for name in os.listdir(directory)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home/k45848/multispectral-imagery-segmentation/data/clean_data\"\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".pkl\")]\n",
    "file_paths\n",
    "\n",
    "for path in file_paths:\n",
    "    to_patches(path, 64, 16)\n",
    "\n",
    "dirs = [\"/home/k45848/multispectral-imagery-segmentation/data/clean_data/train\",\n",
    "       \"/home/k45848/multispectral-imagery-segmentation/data/clean_data/eval\"]\n",
    "\n",
    "for dir in dirs:\n",
    "    count_files_in_directory(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues\n",
    "- Not sure the train/eval split went the way I wanted (randomised)\n",
    "- Still can't count the files in each dir\n",
    "Maybe just do it like before (first split to patches, then copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in dirs:\n",
    "    count_files_in_directory(dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
