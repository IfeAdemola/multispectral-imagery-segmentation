{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import shutil\n",
    "# from plot import * #load_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- For each region get the label of each timestamp\n",
    "- The mask in the region is (0,1) 0 being invalid areas, such as farmlands, 1 being valid areas such as forest area\n",
    "- Within the forest area, there are deforested regions\n",
    "- These regions are taken into consideration by creating a third mask, 2. The mask is generated using the mask channel (11th channel) in the region data, and the 1st and 2nd channels (ID, time stamp) in the areas without trees data\n",
    "- If a pixel is 1 in the region mask (meaning it is part of the valid region) and if it is greater than 0 in the ID channel of the without trees data (meaning it is a deforested area/clearing) and if it is a specific date based on the file name, the clearing was created at that time stamp. \n",
    "- I think I should do less than equal to to take into consideration the deforested areas that already occured in previous time steps and not just new ones in that time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load region data (x, y, 11) and without tree data (x, y, 4) files \n",
    "- Concatenate the 'id','timestamp' and 'veg cover' channels to all 'region_..._final_data' arrays\n",
    "- 'region_..._final_data' arrays now have a shape of (x, y, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not apppend areas without trees to dictionary so len should be 6 and not 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all original .pkl from the root diretory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/k45848/multispectral-imagery-segmentation/data/original_data/region_2_east_2023_05_31_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/original_data/region_2_east_2024_02_25_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/original_data/areas_without_trees_east_raster_final.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/original_data/region_1_west_2023_09_26_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/original_data/region_1_west_2024_02_28_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/original_data/areas_without_trees_west_raster_final.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/original_data/region_2_east_2023_09_28_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/original_data/region_1_west_2023_05_31_final_data.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_file_paths(root_dir):\n",
    "    file_paths = [os.path.join(root_dir, file_name) for file_name in os.listdir(root_dir) if file_name.endswith(\".pkl\")]\n",
    "    return file_paths\n",
    "\n",
    "folder_path = \"/home/k45848/multispectral-imagery-segmentation/data/original_data\"\n",
    "get_file_paths(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .pkl files \n",
    "Append the labels to the multispectral image array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_2_east_2023_05_31_final_data (746, 744, 15)\n",
      "region_2_east_2024_02_25_final_data (746, 744, 15)\n",
      "region_1_west_2023_09_26_final_data (931, 932, 15)\n",
      "region_1_west_2024_02_28_final_data (931, 932, 15)\n",
      "region_2_east_2023_09_28_final_data (746, 744, 15)\n",
      "region_1_west_2023_05_31_final_data (931, 932, 15)\n"
     ]
    }
   ],
   "source": [
    "def get_file_paths(root_dir):\n",
    "    file_paths = [os.path.join(root_dir, file_name) for file_name in os.listdir(root_dir) if file_name.endswith(\".pkl\")]\n",
    "    return file_paths\n",
    "\n",
    "def load_array(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def combine_data_label(root_dir):\n",
    "    \"\"\"Combine the label arrays to the data, so has to create the final label\n",
    "    input  8 files\n",
    "    output 6 files\"\"\"\n",
    "    file_paths = get_file_paths(root_dir=root_dir)\n",
    "\n",
    "    arrays = {}\n",
    "    for path in file_paths:\n",
    "        array = load_array(path)\n",
    "\n",
    "        # Extract file name without extension (.pkl) as key\n",
    "        key = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "        # Store arrays in dictionary (all data and label files)\n",
    "        arrays[key] = array\n",
    "\n",
    "    for key in arrays.keys():\n",
    "        # Check if the key contains 'without_trees'\n",
    "        if 'without_trees' in key:\n",
    "            # Extract cardinal point from the key\n",
    "            cardinal_point = key.split('_')[-3]\n",
    "            \n",
    "            # Find matching keys without 'without_trees' and the same cardinal point\n",
    "            matching_keys = [k for k in arrays.keys() if cardinal_point in k and 'without_trees' not in k]\n",
    "            \n",
    "            # Iterate through matching keys and update arrays\n",
    "            for match_key in matching_keys:\n",
    "                # Add the first two channels from the 'without_trees' array to the matching array\n",
    "                arrays[match_key] = np.concatenate([arrays[match_key], arrays[key][...]], axis=-1)\n",
    "\n",
    "    mask = 'without_trees'\n",
    "    arrays = {key: value for key, value in arrays.items() if mask not in key}\n",
    "\n",
    "    return arrays\n",
    "\n",
    "arrays = combine_data_label(root_dir=folder_path)\n",
    "for key, value in arrays.items():\n",
    "    print(key, value.shape)   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contradictry to the previous explanation, I concatenated all channels in the 'areas without trees' files to their corresponding regional image array\n",
    "- Shape = (x,y,15)\n",
    "- array[:,:10] = 10 spectral channels of image\n",
    "- array[10] = mask created by Denise 1 (valid: forest area), and 0 (invalid: urban areas)\n",
    "- array[11] = id (areas without trees)\n",
    "- array[12] = timestamp (areas without trees)\n",
    "- array[13] = type (areas without trees)\n",
    "- array[14] = veg_cover (areas without trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply condition to get the regions that are deforested for each time stamp\n",
    "- So now channel 11 has 3 unique values (0,1,2) rather than 2 (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.], dtype=float32), array([211064, 343960]))\n",
      "region_2_east_2023_05_31_final_data\n",
      "(array([0., 1.], dtype=float32), array([245556, 309468]))\n",
      "region_2_east_2024_02_25_final_data\n",
      "(array([0., 1.], dtype=float32), array([423854, 443838]))\n",
      "region_1_west_2023_09_26_final_data\n",
      "(array([0., 1.], dtype=float32), array([421453, 446239]))\n",
      "region_1_west_2024_02_28_final_data\n",
      "(array([0., 1.], dtype=float32), array([215878, 339146]))\n",
      "region_2_east_2023_09_28_final_data\n",
      "(array([0., 1.], dtype=float32), array([421453, 446239]))\n",
      "region_1_west_2023_05_31_final_data\n"
     ]
    }
   ],
   "source": [
    "for key, value in arrays.items():\n",
    "    print(np.unique(value[...,10], return_counts=True))\n",
    "    print(key)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(14, 24), match='2023_05_31'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = list(arrays.items())#[:-1]\n",
    "\n",
    "# Add deforested class (1) to channel 11 so it changes from (0,1) to (0,1,2 [invalid, deforested, forest])\n",
    "tsm = []\n",
    "for key, array in items:\n",
    "    time_stamp_match = re.search(r'(\\d{4})_(\\d{2})_(\\d{2})', key)\n",
    "\n",
    "    if time_stamp_match:\n",
    "        ts = float(time_stamp_match.group(1)[2:] + time_stamp_match.group(2) + time_stamp_match.group(3))\n",
    "\n",
    "print(time_stamp_match)\n",
    "type(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(arrays.items())#[:-1]\n",
    "\n",
    "# Add deforested class (1) to channel 11 so it changes from (0,1) to (0,1,2 [invalid, deforested, forest])\n",
    "for key, array in items:\n",
    "    time_stamp_match = re.search(r'(\\d{4})_(\\d{2})_(\\d{2})', key)\n",
    "\n",
    "    if time_stamp_match:\n",
    "        ts = float(time_stamp_match.group(1)[2:] + time_stamp_match.group(2) + time_stamp_match.group(3))  #gets the ts (date)\n",
    "\n",
    "\n",
    "    condition = (array[..., 10] == 1) & (array[..., 11] > 0) & (array[..., 12] <= ts)\n",
    "    \n",
    "    array[condition, 10] = 2\n",
    "\n",
    "\n",
    "    # print(np.unique(array[..., -1], return_counts=True))\n",
    "\n",
    "    # Make a new channel where deforested class pixels are split into their different vegetation cover\n",
    "    # Add a new channel, initialise to zero\n",
    "    veg_mask = np.zeros(array.shape[:-1] +(1,))\n",
    "    array = np.concatenate((array, veg_mask), axis=-1)\n",
    "\n",
    "    condition_1 = (array[..., 10] == 1)\n",
    "    array[condition_1, 15] = 5\n",
    "\n",
    "    condition_2 = (array[..., 10] == 2) & (array[..., 14] == 1)\n",
    "    array[condition_2, 15] = 1\n",
    "\n",
    "    condition_3 = (array[..., 10] == 2) & (array[..., 14] == 2)\n",
    "    array[condition_3, 15] = 2\n",
    "\n",
    "    condition_4 = (array[..., 10] == 2) & (array[..., 14] == 3)\n",
    "    array[condition_4, 15] = 3\n",
    "\n",
    "    condition_5 = (array[..., 10] == 2) & (array[..., 14] == 4)\n",
    "    array[condition_5, 15] = 4\n",
    "\n",
    "    \"\"\"At this point array has 16 channels: 10 spectral, 1 mask, 4 attributes and 1 veg mask\"\"\"\n",
    "    # Create a new channel to take into account the weather season, making it 17 channels\n",
    "    if '2023_05'  in key:\n",
    "        season_mask = np.zeros(array.shape[:-1]+ (1,))\n",
    "    elif '2023_09' in key:\n",
    "        season_mask = np.ones(array.shape[:-1]+ (1,))\n",
    "    elif '2024_02' in key:\n",
    "        season_mask = np.full(array.shape[:-1]+ (1,), 2)     \n",
    "\n",
    "    array = np.concatenate((array, season_mask), axis=-1)\n",
    "\n",
    "    # Delete the attribute channels (id, timestamp, ...)\n",
    "    # Should have just 13 channels left\n",
    "    array = np.delete(array, [11,12,13,14], axis=-1)\n",
    "\n",
    "    arrays[key] = array\n",
    "\n",
    "    # filename = f\"/home/k45848/multispectral-imagery-segmentation/data/clean_data/{key}.pkl\"\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(array, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check shape, of array, unique values e.t.c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_2_east_2023_05_31_final_data (746, 744, 13)\n",
      "region_2_east_2024_02_25_final_data (746, 744, 13)\n",
      "region_1_west_2023_09_26_final_data (931, 932, 13)\n",
      "region_1_west_2024_02_28_final_data (931, 932, 13)\n",
      "region_2_east_2023_09_28_final_data (746, 744, 13)\n",
      "region_1_west_2023_05_31_final_data (931, 932, 13)\n"
     ]
    }
   ],
   "source": [
    "for key, value in arrays.items():\n",
    "    print(key, value.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in arrays.items():\n",
    "#     print(np.unique(value[...,10], return_counts=True))\n",
    "\n",
    "# for key, value in arrays.items():\n",
    "#     print(np.unique(value[...,11], return_counts=True))\n",
    " \n",
    "# for key, value in arrays.items():\n",
    "    # print(key)\n",
    "    # print(np.unique(value[...,12], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2., 3., 5.]), array([211064,   1159,    316,    202, 342283]))\n",
      "(array([0., 1., 2., 3., 5.]), array([245556,   6706,    399,    202, 302161]))\n",
      "(array([0., 1., 2., 3., 4., 5.]), array([423854,   4149,    861,    445,   1431, 436952]))\n",
      "(array([0., 1., 2., 3., 4., 5.]), array([421453,   4442,    861,    445,   1431, 439060]))\n",
      "(array([0., 1., 2., 3., 5.]), array([215878,   6208,    399,    202, 332337]))\n",
      "(array([0., 1., 2., 3., 4., 5.]), array([421453,   1011,    651,    445,   1431, 442701]))\n",
      "region_2_east_2023_05_31_final_data\n",
      "(array([0., 1., 2.]), array([211064, 342283,   1677]))\n",
      "region_2_east_2024_02_25_final_data\n",
      "(array([0., 1., 2.]), array([245556, 302161,   7307]))\n",
      "region_1_west_2023_09_26_final_data\n",
      "(array([0., 1., 2.]), array([423854, 436952,   6886]))\n",
      "region_1_west_2024_02_28_final_data\n",
      "(array([0., 1., 2.]), array([421453, 439060,   7179]))\n",
      "region_2_east_2023_09_28_final_data\n",
      "(array([0., 1., 2.]), array([215878, 332337,   6809]))\n",
      "region_1_west_2023_05_31_final_data\n",
      "(array([0., 1., 2.]), array([421453, 442701,   3538]))\n"
     ]
    }
   ],
   "source": [
    "# Swap the 10th and 12th channels\n",
    "#Move the season (date) mask to the 10th space cuz it is an input feature and not a target. For convinience\n",
    "def swap_channels(arr):\n",
    "    arr[:,:,[10, 12]] = arr[:,:,[12, 10]]\n",
    "    return arr\n",
    "\n",
    "# for key, value in arrays.items():\n",
    "#     v = swap_channels(value)\n",
    "final_arrays = {key: swap_channels(value) for key, value in arrays.items()}\n",
    "# for key, value in arrays.items():\n",
    "#     print(np.unique(value[...,10], return_counts=True))\n",
    "\n",
    "for key, value in final_arrays.items():\n",
    "    print(np.unique(value[...,11], return_counts=True))\n",
    " \n",
    "for key, value in final_arrays.items():\n",
    "    print(key)\n",
    "    print(np.unique(value[...,12], return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"/home/k45848/multispectral-imagery-segmentation/data/27.05/{key}.pkl\"\n",
    "for k, v in final_arrays.items():\n",
    "    with open(f\"/home/k45848/multispectral-imagery-segmentation/data/27.05/{k}.pkl\", 'wb') as file:\n",
    "        pickle.dump(v, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split arrays into patches and distribute patches randomly to train and eval directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_patches(file_path: str, patch_size: int, stride: int) -> None:\n",
    "    \"\"\"\n",
    "    Convert an array to patches of a given size and stride and save them to train or eval directory.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the input array file.\n",
    "    - patch_size (int): Size of each patch.\n",
    "    - stride (int): Stride between patches.\n",
    "    - train_dir (str): Directory to save train patches.\n",
    "    - eval_dir (str): Directory to save eval patches.\n",
    "    \"\"\"\n",
    "    array = load_array(file_path)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]  # Get file name without extension\n",
    "\n",
    "    height, width, _ =  array.shape\n",
    "    patch_height, patch_width = patch_size, patch_size\n",
    "    stride_height, stride_width = stride, stride\n",
    "\n",
    "    # Calculate the number of patches in each dimension\n",
    "    num_patches_height = (height - patch_height) // stride_height + 1\n",
    "    num_patches_width = (width - patch_width) // stride_width + 1\n",
    "\n",
    "    # Determine the directory based on a random choice for train or eval\n",
    "    base_dir =os.path.dirname(file_path)\n",
    "    train_dir = f\"{base_dir}/train/\"\n",
    "    eval_dir = f\"{base_dir}/eval/\"\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    if not os.path.exists(eval_dir):\n",
    "        os.makedirs(eval_dir)\n",
    "\n",
    "\n",
    "    # dir = train_dir if random.choice([True, False]) else eval_dir\n",
    "    # Determine the directory based on the 80:20 split\n",
    "    # dir = train_dir if random.random() < 0.8 else eval_dir\n",
    "    # os.makedirs(dir)\n",
    "\n",
    "    for i in range(num_patches_height):\n",
    "        for j in range(num_patches_width):\n",
    "            start_i = i * stride_height\n",
    "            start_j = j * stride_width\n",
    "            patch = array[start_i:start_i + patch_height, start_j:start_j + patch_width, :]\n",
    "\n",
    "            dir = train_dir if random.random() < 0.8 else eval_dir\n",
    "            patch_filename = f\"{dir}{file_name}_{i* num_patches_width + j + 1}.pkl\"\n",
    "             \n",
    "            with open(patch_filename, 'wb') as f:\n",
    "                pickle.dump(patch, f)\n",
    "\n",
    "\n",
    "folder_path = \"/home/k45848/multispectral-imagery-segmentation/data/27.05\"\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".pkl\")]\n",
    "for path in file_paths:\n",
    "    to_patches(path, patch_size=64, stride=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/k45848/multispectral-imagery-segmentation/data/clean_data/region_2_east_2023_05_31_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/clean_data/region_2_east_2024_02_25_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/clean_data/region_1_west_2023_09_26_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/clean_data/region_1_west_2024_02_28_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/clean_data/region_2_east_2023_09_28_final_data.pkl',\n",
       " '/home/k45848/multispectral-imagery-segmentation/data/clean_data/region_1_west_2023_05_31_final_data.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the folder: 2989\n",
      "Number of files in the folder: 11633\n"
     ]
    }
   ],
   "source": [
    "def count_files(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    file_count = len(files)\n",
    "    return file_count\n",
    "\n",
    "folder_path = [\"/home/k45848/multispectral-imagery-segmentation/data/27.05/eval\",\n",
    "               \"/home/k45848/multispectral-imagery-segmentation/data/27.05/train\"]\n",
    "for f in folder_path:\n",
    "    num_files = count_files(f)\n",
    "    print(\"Number of files in the folder:\", num_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues\n",
    "- Not sure the train/eval split went the way I wanted (randomised)\n",
    "- ~~Still can't count the files in each dir\n",
    "Maybe just do it like before (first split to patches, then copy)~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
